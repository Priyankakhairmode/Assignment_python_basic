{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " 1. What is TensorFlow 2.0, and how is it different from TensorFlow 1.x2\n",
        "Ans: TensorFlow 2.0 aims to provide a more Pythonic, user-friendly, and efficient experience for machine learning development, particularly for beginners and researchers, while still offering the power and flexibility required for advanced applications.\n",
        "Eager Execution by Default:\n",
        "TensorFlow 2.0: Eager execution is enabled by default, meaning operations are executed immediately, similar to standard Python or NumPy. This simplifies debugging and makes the code more interactive.\n",
        "TensorFlow 1.x: Required building a static computational graph first and then executing it within a tf.Session, which could make debugging more challenging.\n",
        "Keras Integration:\n",
        "TensorFlow 2.0: Keras is the default high-level API for building and training models, offering a streamlined and user-friendly experience.\n",
        "TensorFlow 1.x: Keras integration was optional and less seamless, often requiring more verbose code.\n",
        "API Simplification and Consolidation:\n",
        "TensorFlow 2.0: Consolidates many fragmented APIs into a more consistent and intuitive set, reducing boilerplate code and making it easier to learn and use.\n",
        "TensorFlow 1.x: Had a more fragmented API structure with multiple ways to achieve similar tasks.\n",
        "Automatic Differentiation with tf.GradientTape:\n",
        "TensorFlow 2.0: Introduces tf.GradientTape for dynamic computation of gradients, simplifying the implementation of custom training loops and advanced optimization algorithms.\n",
        "TensorFlow 1.x: Lacked a similarly sophisticated and user-friendly automatic differentiation system.\n",
        "No More Globals and Sessions:\n",
        "TensorFlow 2.0: Eliminates the need for global variables and explicit sessions, leading to cleaner and more organized code.\n",
        "TensorFlow 1.x: Relied heavily on global collections and session management."
      ],
      "metadata": {
        "id": "JUTiiIckvJpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 How do you install TensorFlow 2.02\n",
        "To install a specific version like TensorFlow 2.0.2, you should use Python's pip package manager within a virtual environment.\n",
        "Prerequisites\n",
        "Python: Ensure you have Python installed. TensorFlow 2.0.2 is compatible with Python 3.5 to 3.8.\n",
        "Pip: Make sure your pip version is recent (version >19.0).\n",
        "Virtual Environment (Recommended): Use a virtual environment (venv or conda) to avoid conflicts with other Python projects.\n",
        "Installation Steps (using a Python Virtual Environment)\n",
        "Create a Virtual Environment:\n",
        "Open your terminal or command prompt and run the following commands to create a new environment named tf_env:\n",
        "bash\n",
        "python3 -m venv tf_env\n",
        "(On Windows, you might use python instead of python3).\n",
        "Activate the Virtual Environment:\n",
        "macOS/Linux: source tf_env/bin/activate\n",
        "Windows: tf_env\\Scripts\\activate\n",
        "Your command prompt will show the environment name in parentheses (e.g., (tf_env)), indicating it is active.\n",
        "Upgrade Pip:\n",
        "Ensure your pip is up-to-date within the activated environment:\n",
        "bash\n",
        "pip install --upgrade pip\n",
        "Install TensorFlow 2.0.2:\n",
        "Install the specific version you need using pip:\n",
        "bash\n",
        "pip install tensorflow==2.0.2\n",
        "This command installs the CPU-only version.\n",
        "Verify the Installation:\n",
        "Run a simple Python command to check the installed version and ensure it works:\n",
        "bash\n",
        "python -c \"import tensorflow as tf; print(tf.__version__)\""
      ],
      "metadata": {
        "id": "KwQY-EJEvpE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the primary function of the tf.function in TensorFlow 2.02\n",
        "The primary function of tf.function is to transform a standard Python function containing TensorFlow operations into a portable, high-performance, and Python-independent dataflow graph. This conversion allows TensorFlow to apply graph-level optimizations, exploit parallelism, and improve execution speed, particularly for large-scale computations and deployment."
      ],
      "metadata": {
        "id": "NyuwchJhwOjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 What is the purpose of the Model class in TensorFlow 2.02\n",
        "The purpose of the tf.keras.Model class in TensorFlow 2.x is to group layers into an object with training and inference features, providing a high-level API for building, training, and saving machine learning models.\n",
        "Specifically, the Model class serves several key purposes:\n",
        "Encapsulation of Layers: It allows you to logically group together multiple tf.keras.layers.Layer instances, forming a complete neural network architecture. This can be done through the Functional API or by subclassing the Model class.\n",
        "Built-in Training and Evaluation Loops: The Model class provides convenient methods like model.fit(), model.evaluate(), and model.predict() for handling the entire training, evaluation, and inference process with minimal code.\n",
        "Model Saving and Serialization: It offers built-in functionalities for saving the model's architecture and weights (model.save(), model.save_weights()) and loading them back, enabling persistence and reusability of trained models.\n",
        "Managing Model State: It automatically tracks the model's layers and their trainable weights, making it easier to manage the model's internal state during training and inference.\n",
        "Flexibility in Model Construction: While tf.keras.Sequential is suitable for simple, linear stacks of layers, the Model class (especially through the Functional API or subclassing) provides the flexibility to build more complex architectures with multiple inputs, multiple outputs, and shared layers."
      ],
      "metadata": {
        "id": "x-aIs48WwkgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do you create a neural network using TensorFlow 2.02\n",
        "Creating a neural network in TensorFlow 2.x typically involves using the Keras API, which is integrated within TensorFlow. The process can be broken down into several key steps:\n",
        "1. Import TensorFlow and Keras:\n",
        "Python\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "2. Prepare your Data:\n",
        "This involves loading your dataset, pre-processing it (e.g., normalization, one-hot encoding for categorical labels), and splitting it into training and testing sets.\n",
        "3. Define the Model Architecture:\n",
        "You can define a sequential model by stacking layers, or a more complex functional API model for multi-input/output or shared-layer architectures.\n",
        "Python\n",
        "\n",
        "# Sequential Model Example\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),  # Input layer for 28x28 images\n",
        "    keras.layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n",
        "    keras.layers.Dense(10, activation='softmax') # Output layer for 10 classes with Softmax\n",
        "])\n",
        "4. Compile the Model:\n",
        "This step configures the model for training by specifying the optimizer, loss function, and metrics.\n",
        "Python\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "5. Train the Model:\n",
        "Use the fit method to train the model on your training data. You'll specify the training data (features and labels), the number of epochs, and optionally a validation set.\n",
        "Python\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
        "6. Evaluate the Model:\n",
        "Assess the model's performance on unseen data using the evaluate method.\n",
        "Python\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy}\")\n",
        "7. Make Predictions (Optional):\n",
        "Once trained, you can use the predict method to get predictions on new data.\n",
        "Python\n",
        "\n",
        "predictions = model.predict(new_data)"
      ],
      "metadata": {
        "id": "ofFjJmVfwzAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the importance of Tensor Space in TensorFlow2\n",
        "AI Overview\n",
        "In TensorFlow 2.x, tensors are the fundamental data structures used to represent and manipulate all forms of data, from raw inputs to model parameters and outputs. The concept of \"tensor space\" refers to this environment where multi-dimensional numerical data is efficiently processed, forming the backbone of all machine learning computations.\n",
        "The importance of the tensor space in TensorFlow 2.x lies in several key aspects:\n",
        "Universal Data Representation: Tensors generalize scalars (rank-0), vectors (rank-1), and matrices (rank-2) to arbitrary higher dimensions. This allows TensorFlow to handle diverse data types like images (4D tensors with batch, height, width, channels), text data, and complex tabular data within a unified framework.\n",
        "Mathematical Operations and Computation: The tensor space provides a robust framework for performing a vast array of mathematical operations, including element-wise addition, matrix multiplication, and complex transformations like convolutions. These operations are essential for building and executing machine learning models.\n",
        "Hardware Acceleration: Tensors are optimized for high-performance numerical computation and can seamlessly operate across different devices, including CPUs, GPUs, and Google's custom Tensor Processing Units (TPUs). This ability to leverage specialized hardware for parallel processing is crucial for speeding up the training and inference of large deep learning models.\n",
        "Automatic Differentiation: Tensors in TensorFlow 2.x support automatic differentiation via the tf.GradientTape API. This feature is vital for the backpropagation algorithm used in training neural networks, as it automatically calculates the gradients needed to optimize model parameters (weights and biases).\n",
        "Flexibility and Efficiency: TensorFlow operations often return new, immutable tensors, while memory is managed efficiently behind the scenes. Operations like tf.reshape() are fast because they typically just create a new tensor with the requested shape pointing to the same underlying data, without duplicating it in memory.\n",
        "Integration with the Keras API: Tensors are the primary data format used by the high-level Keras API, which is the recommended way to build models in TensorFlow 2.x. Keras abstracts away much of the low-level tensor manipulation, allowing developers to focus on model architecture while the framework handles tensor flow and operations automatically."
      ],
      "metadata": {
        "id": "HRXAJjufxYwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How can TensorBoard be integrated with TensorFlow 2.02\n",
        "Integrating TensorBoard with TensorFlow 2.x, including version 2.02, primarily involves using the tf.keras.callbacks.TensorBoard callback for Keras models or tf.summary for custom training loops.\n",
        "For Keras Models:\n",
        "Import TensorBoard: Import the TensorBoard callback from Keras.\n",
        "Python\n",
        "\n",
        "    from tensorflow.keras.callbacks import TensorBoard\n",
        "Define a Log Directory: Specify a directory where TensorBoard logs will be saved. It's common practice to use a timestamped subdirectory for each run.\n",
        "Python\n",
        "\n",
        "    import datetime\n",
        "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "Create a TensorBoard Callback Instance: Instantiate the TensorBoard callback, passing the log_dir. You can also configure other options like histogram_freq to log weight and bias histograms.\n",
        "Python\n",
        "\n",
        "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "Pass the Callback to model.fit(): When training your Keras model, include the tensorboard_callback in the callbacks list.\n",
        "Python\n",
        "\n",
        "    model.fit(x_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n",
        "For Custom Training Loops (using tf.GradientTape):\n",
        "Create a Summary Writer: Create tf.summary.create_file_writer objects for logging training and validation metrics.\n",
        "Python\n",
        "\n",
        "    import tensorflow as tf\n",
        "    import datetime\n",
        "    log_dir = \"logs/gradient_tape/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_writer = tf.summary.create_file_writer(log_dir + '/train')\n",
        "    test_writer = tf.summary.create_file_writer(log_dir + '/test')\n",
        "Log Metrics within tf.summary.SummaryWriter Scope: Inside your training loop, use with train_writer.as_default(): or with test_writer.as_default(): to log scalars, images, or other data using tf.summary functions (e.g., tf.summary.scalar(), tf.summary.image()).\n",
        "Python\n",
        "\n",
        "    # Example within a training step\n",
        "    with train_writer.as_default():\n",
        "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "Launching TensorBoard:\n",
        "After generating logs, launch TensorBoard from your terminal or within a Jupyter Notebook/Colab environment: Terminal.\n",
        "Code\n",
        "\n",
        "    tensorboard --logdir logs/fit  # or logs/gradient_tape\n",
        "Jupyter/Colab.\n",
        "Python\n",
        "\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir logs/fit # or logs/gradient_tape"
      ],
      "metadata": {
        "id": "tRNWtlHrxvhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the purpose of TensorFlow Playground2\n",
        "TensorFlow Playground's purpose is to provide a visual and interactive way to learn about and experiment with neural networks without needing to write code. It allows users to adjust hyperparameters and network structures in real-time to see how they affect the model's performance on classification and regression tasks. It's a tool for understanding fundamental concepts like neurons, layers, activation functions, and how training parameters impact a model's ability to learn."
      ],
      "metadata": {
        "id": "VjUCqdbMyAPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is Netron, and how is it useful for deep learning models2\n",
        "In the context of deep learning and artificial intelligence (AI), \"neutron\" typically refers to data derived from neutron-based scientific techniques (like neutron scattering, diffraction, and reflectometry) that are then processed using deep learning models.\n",
        "The unique properties of neutrons make them a powerful tool to probe the atomic-scale structure and dynamics of materials. They are complementary to X-rays because they interact with the nucleus rather than the electron cloud, are sensitive to magnetism, and can easily penetrate dense matter.\n",
        "Deep learning is applied to data from neutron experiments to address several challenges:\n",
        "Accelerated Data Analysis: Traditional analysis of neutron data is complex and time-consuming, requiring significant expertise. Deep learning models (like Convolutional Neural Networks, or CNNs) automate and speed up tasks such as identifying crystal structures from diffraction patterns.\n",
        "Signal Enhancement/Noise Reduction: Neutron sources often have a lower flux compared to X-ray sources, resulting in data with high statistical noise or requiring long acquisition times. Deep learning models, such as denoising CNNs, can restore high-quality signals from noisy data, effectively reducing the required experimental time significantly.\n",
        "Experiment Optimization: Machine learning models can help scientists interpret data in real-time, allowing for early decisions and the optimization of instrument settings and measurement strategies, leading to more efficient use of limited beam time.\n",
        "Complex Modeling: Deep learning is used in areas like nuclear physics to model the equation of state of dense neutron-rich matter (like in neutron stars) from observational data, a task challenging for traditional methods."
      ],
      "metadata": {
        "id": "7Gf9t6TzyURM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between TensorFlow and PyTorch2\n",
        "TensorFlow and PyTorch are prominent open-source deep learning frameworks, each with distinct characteristics that cater to different use cases and preferences.\n",
        "1. Computational Graph:\n",
        "PyTorch: Utilizes a dynamic computational graph, which means the graph is built on-the-fly during execution. This offers flexibility for debugging and allows for dynamic model architectures, making it popular for research and rapid prototyping.\n",
        "TensorFlow: Primarily uses a static computational graph, where the graph is defined before execution. This can lead to better optimization and performance in large-scale production deployments, but requires recompilation for changes to the graph. TensorFlow 2.x, however, has introduced more imperative programming features with tf.function and Keras integration, blurring this distinction somewhat.\n",
        "2. Usability and Python Integration:\n",
        "PyTorch: Is often considered more \"Pythonic\" and intuitive for Python developers due to its imperative programming style and close integration with standard Python libraries like NumPy. This makes it easier to learn for those already familiar with Python.\n",
        "TensorFlow: Historically, TensorFlow had a steeper learning curve, but TensorFlow 2.x, with its emphasis on Keras as a high-level API, has significantly improved ease of use and user-friendliness.\n",
        "3. Deployment and Production:\n",
        "TensorFlow: Offers robust deployment options for various platforms, including mobile (TensorFlow Lite), web (TensorFlow.js), and production serving (TensorFlow Serving). It is often favored for large-scale industrial applications.\n",
        "PyTorch: While improving, its deployment ecosystem is less extensive than TensorFlow's, often requiring more custom solutions for production deployment.\n",
        "4. Community and Ecosystem:\n",
        "TensorFlow: Has a mature and extensive community, backed by Google, with a vast array of tools, libraries, and pre-trained models within its ecosystem.\n",
        "PyTorch: Has a rapidly growing community, particularly in academic research, with increasing industry adoption.\n",
        "5. Debugging:\n",
        "PyTorch: The dynamic graph in PyTorch facilitates easier debugging, as standard Python debugging tools can be used to inspect intermediate values during execution.\n",
        "TensorFlow: Debugging static graphs can be more challenging, although tools like TensorBoard offer powerful visualization and profiling capabilities."
      ],
      "metadata": {
        "id": "5znFaOj8y9ZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. How do you install PyTorch2\n",
        "To install PyTorch 2, go to the official PyTorch website and use the command generator to create a specific installation command for your system, which you will then run in your terminal or command prompt. The command will depend on your operating system, chosen package manager (like pip or conda), and whether you are using a GPU (like CUDA) or the CPU"
      ],
      "metadata": {
        "id": "DqUBh_pWzMnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the basic structure of a PyTorch neural network2\n",
        "The basic structure of a PyTorch neural network revolves around the torch.nn.Module class. This class serves as the base for all neural network modules, including individual layers and entire network architectures.\n",
        "Here's a breakdown of the key components:\n",
        "torch.nn.Module:\n",
        "All neural networks and their layers in PyTorch inherit from nn.Module.\n",
        "It provides functionality for managing parameters (weights and biases), moving the model to different devices (CPU/GPU), and handling the forward pass.\n",
        "Layers (Sub-modules):\n",
        "A neural network is typically composed of various layers, each also inheriting from nn.Module.\n",
        "Common layers include:\n",
        "nn.Linear: A fully connected layer that applies a linear transformation.\n",
        "nn.Conv2d: A convolutional layer for processing image data.\n",
        "nn.ReLU, nn.Sigmoid, nn.Tanh: Activation functions that introduce non-linearity.\n",
        "nn.MaxPool2d: A pooling layer for downsampling.\n",
        "nn.Dropout: A regularization layer to prevent overfitting.\n",
        "__init__ Method:\n",
        "Within your custom neural network class (which inherits from nn.Module), the __init__ method is used to define and initialize the layers and other components of your network.\n",
        "You call super().__init__() to properly initialize the base nn.Module class.\n",
        "forward Method:\n",
        "This method defines the forward pass of your neural network. It dictates how data flows through the layers and how computations are performed to produce the output.\n",
        "You apply the defined layers and activation functions to the input data in this method."
      ],
      "metadata": {
        "id": "ie_Hfs0NzWMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What is the significance of tensors in PyTorch2\n",
        "Tensors are the fundamental data structure in PyTorch, acting as multi-dimensional arrays that store data for neural networks, such as inputs, outputs, and parameters. Their significance lies in their ability to be accelerated by hardware like GPUs, perform automatic differentiation for gradient computation, and support a wide range of mathematical operations for building and training models.\n",
        "Key significance of tensors in PyTorch\n",
        "Hardware acceleration: Tensors can be moved to and processed on a GPU, which is crucial for the speed of deep learning training and inference.\n",
        "Automatic differentiation: PyTorch's autograd engine works seamlessly with tensors, enabling automatic computation of gradients (derivatives), a core component of training neural networks.\n",
        "Universal data structure: Tensors can represent various data types, from simple scalars to complex multi-dimensional data like images and time series, making them a versatile tool for all parts of a machine learning pipeline.\n",
        "Model building blocks:\n",
        "Inputs/Outputs: Tensors encode the data fed into a neural network and the results it produces.\n",
        "Parameters: Tensors hold the learnable parameters of a model, such as weights and biases.\n",
        "Interoperability: Tensors can be easily created from or converted to NumPy arrays, allowing for seamless integration with other scientific libraries.\n",
        "Rich operation support: PyTorch provides a vast library of functions and operations to manipulate tensors, including common mathematical functions, bitwise operations, and comparisons."
      ],
      "metadata": {
        "id": "QIcelFaLziA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8tRQYchFyyve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch2\n",
        "The fundamental difference between torch.Tensor and torch.cuda.Tensor in PyTorch lies in the device on which they are allocated and the operations are performed.\n",
        "torch.Tensor: This is the general PyTorch tensor type, and by default, it is allocated on the CPU memory. Operations involving torch.Tensor are computed using the CPU.\n",
        "torch.cuda.Tensor: This is a specialized tensor type that is explicitly allocated on the GPU memory (specifically, using NVIDIA's CUDA platform). Operations involving torch.cuda.Tensor are computed using the GPU, which can offer significant speedups for computationally intensive tasks like deep learning.\n",
        "Key distinctions:\n",
        "Memory Location: torch.Tensor resides in CPU RAM, while torch.cuda.Tensor resides in GPU VRAM.\n",
        "Computational Device: Operations on torch.Tensor are executed by the CPU, whereas operations on torch.cuda.Tensor are executed by the GPU.\n",
        "Performance: For large-scale numerical computations, especially in deep learning, torch.cuda.Tensor typically offers much faster performance due to the parallel processing capabilities of GPUs.\n",
        "Creation and Transfer: You can create a torch.Tensor directly, and then transfer it to the GPU using methods like .to('cuda') or .cuda(). This conversion results in a torch.cuda.Tensor."
      ],
      "metadata": {
        "id": "nl7RjBk3z1Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the purpose of the torch.optim module in PyTorch\n",
        "The torch.optim module in PyTorch provides various optimization algorithms crucial for training neural networks. Its primary purpose is to update the weights and biases (parameters) of a neural network in a way that minimizes the loss function.\n",
        "Here's a breakdown of its key functions:\n",
        "Minimizing Loss: During the training of a neural network, a loss function quantifies the discrepancy between the network's predictions and the actual target values. The torch.optim module offers algorithms (optimizers) that iteratively adjust the model's parameters to reduce this loss, thereby improving the model's performance.\n",
        "Implementing Optimization Algorithms: It encapsulates a wide range of commonly used optimization algorithms in deep learning, such as:\n",
        "Stochastic Gradient Descent (SGD): Including variants with momentum.\n",
        "Adam: A popular adaptive learning rate optimization algorithm.\n",
        "RMSprop: Another adaptive learning rate algorithm.\n",
        "Adagrad, Adadelta, AdamW, LBFGS, NAdam, RAdam, etc.\n",
        "Automating Parameter Updates: Instead of manually calculating and applying updates to each parameter, torch.optim allows you to define an Optimizer instance that handles this process automatically. After computing the gradients of the loss with respect to the parameters, a single call to the optimizer's step() method updates all the parameters.\n",
        "Gradient Management: Optimizers also provide methods like zero_grad() to reset the gradients before computing them for a new mini-batch, preventing unintended accumulation of gradients from previous iterations."
      ],
      "metadata": {
        "id": "_FDnGmAEz-Ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What are some common activation functions used in neural networks2\n",
        "Common activation functions include Sigmoid, Tanh, and ReLU, which are popular non-linear functions used in neural networks. Other common types are Softmax, which is used for multi-class classification, Linear, and Binary Step. Non-linear activation functions Sigmoid: An \"S-shaped\" function that squashes the output to a range between 0 and 1, often used for binary classification tasks.Tanh (Hyperbolic Tangent): Also an \"S-shaped\" function, but it squashes the output to a range between -1 and 1, making it symmetric around zero.ReLU (Rectified Linear Unit): Outputs the input directly if it is positive, and outputs zero otherwise (\\(f(x)=max(0,x)\\)). It is computationally efficient and widely used in hidden layers.Leaky ReLU: A variation of ReLU that allows a small, non-zero gradient for negative inputs, helping to prevent the \"dying ReLU\" problem.Softmax: Used in the output layer for multi-class classification problems, it converts the outputs into a probability distribution where all the values sum to 1."
      ],
      "metadata": {
        "id": "LQ-DmF-O0O53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch2\n",
        "torch.nn.Module and torch.nn.Sequential are both fundamental components for building neural networks in PyTorch, but they serve different purposes.\n",
        "torch.nn.Module:\n",
        "Base Class for all Neural Network Modules: nn.Module is the fundamental building block for all neural network components in PyTorch, including layers, entire models, and even custom operations.\n",
        "Flexibility and Customization: When you define a class that inherits from nn.Module, you gain complete control over the network's architecture and forward pass logic. You must explicitly define the __init__ method to declare the layers and the forward method to define how data flows through these layers.\n",
        "Complex Architectures: It is used when you need to create complex architectures with branching paths, skip connections, or custom logic that cannot be represented by a simple linear sequence of layers.\n",
        "torch.nn.Sequential:\n",
        "Container for Sequential Modules: nn.Sequential is a specialized subclass of nn.Module designed for building neural networks where layers are arranged in a strictly sequential order.\n",
        "Simplicity and Conciseness: It provides a convenient and concise way to define simple feed-forward networks by stacking layers one after another. You pass an ordered list of modules (e.g., nn.Linear, nn.ReLU, nn.Conv2d) to its constructor.\n",
        "Automatic Forward Pass: nn.Sequential automatically handles the forward pass by passing the output of one module as the input to the next, eliminating the need to explicitly write a forward method.\n",
        "Limited Flexibility: Its primary limitation is that it can only represent linear pipelines. If your network requires any non-sequential operations or custom data flow, nn.Sequential is not suitable."
      ],
      "metadata": {
        "id": "9p5UcnUS0WIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. How can you monitor training progress in TensorFlow 2.02\n",
        "Monitoring training progress in TensorFlow 2.x can be achieved through several methods:\n",
        "TensorBoard: This is a powerful visualization tool integrated with TensorFlow.\n",
        "TensorBoard Callback: When using model.fit(), you can utilize the tf.keras.callbacks.TensorBoard callback to automatically log metrics like loss, accuracy, and learning rate.\n",
        "Launch TensorBoard: After logging, navigate to the directory containing your logs in a terminal and run tensorboard --logdir <your_log_directory>. Access the provided URL in your browser to view interactive dashboards.\n",
        "Visualizations: TensorBoard allows you to visualize scalar metrics, model graphs, histograms and distributions of weights and biases, and even profile model performance with the Profiler tab.\n",
        "Custom Callbacks: For more granular control or specific monitoring needs, you can create custom callbacks by inheriting from tf.keras.callbacks.Callback.\n",
        "Override Methods: Implement methods like on_epoch_end, on_batch_end, or on_train_begin to log custom metrics, perform actions based on training progress (e.g., early stopping), or print specific information.\n",
        "Progress Bars: For real-time visual feedback during training loops, especially in custom training loops, you can use:\n",
        "tf.keras.utils.Progbar: This built-in utility can display a progress bar indicating the completion of batches within an epoch.\n",
        "External Libraries (e.g., tqdm): Libraries like tqdm can also be integrated into your training loops to provide visually appealing and informative progress bars."
      ],
      "metadata": {
        "id": "OC3cn5MI0fsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 19. How does the Keras API fit into TensorFlow 2.02\n",
        " Keras is the official high-level API for TensorFlow 2.0 and later versions. This means that Keras is deeply integrated into TensorFlow, serving as the primary way to build and train machine learning models within the TensorFlow ecosystem.\n",
        "Here's how Keras fits into TensorFlow 2.0:\n",
        "Integrated as tf.keras: Instead of being a separate library you install and import independently, Keras is available directly as a submodule within TensorFlow, accessed via tf.keras. This ensures seamless compatibility and leverages TensorFlow's backend capabilities.\n",
        "High-Level Abstraction: Keras provides a user-friendly and intuitive interface for building deep learning models. It simplifies complex operations by offering pre-built components like layers, optimizers, and loss functions, allowing users to focus on model architecture rather than low-level TensorFlow operations.\n",
        "Default API for Deep Learning: TensorFlow 2.0 adopted Keras as its recommended and primary API for most deep learning development. This standardization streamlines the development process and promotes consistency across projects.\n",
        "Leverages TensorFlow Features: Keras models built with tf.keras can seamlessly utilize advanced TensorFlow features like eager execution, tf.data pipelines for efficient data handling, distribution strategies for multi-GPU or multi-device training, and SavedModel format for easy deployment.\n",
        "Flexibility and Extensibility: While Keras prioritizes ease of use, it also offers flexibility for advanced users. You can use the Sequential API for simple models, the Functional API for more complex graph-like architectures, or even subclass tf.keras.Model for complete customization and imperative control over the model's forward pass.\n",
        "Production Readiness: Keras models can be easily exported to the TensorFlow SavedModel format, enabling deployment with TensorFlow Serving, TensorFlow Lite for mobile and embedded devices, and TensorFlow.js for web applications."
      ],
      "metadata": {
        "id": "iv-b9IoC0s1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is an example of a deep learning project that can be implemented using TensorFlow 2.02\n",
        "An excellent example of a deep learning project that can be implemented using TensorFlow 2.x is real-time object detection, such as building a face mask detection system. This project leverages Convolutional Neural Networks (CNNs) and the TensorFlow Object Detection API to identify and locate multiple objects within an image or video stream\n",
        "Image Classification: A foundational project that classifies entire images into categories (e.g., identifying images of flowers or different animals) using the tf.keras API.\n",
        "Sentiment Analysis: Building an NLP model (often using RNNs or LSTMs) to classify text reviews or social media posts as having positive, negative, or neutral sentiment.\n",
        "Image Caption Generator: A more advanced project that combines a CNN (for image feature extraction) and an RNN (for text generation) to create descriptive sentences for images."
      ],
      "metadata": {
        "id": "zAJzFH3_04UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 21. What is the main advantage of using pre-trained models in TensorFlow and PyTorch?\n",
        " The main advantage of using pre-trained models in TensorFlow and PyTorch is a significant reduction in training time and computational costs [1, 2].\n",
        "Key benefits include:\n",
        "Reduced Training Time: Instead of training a complex model from scratch for days or weeks on massive datasets, you can use a model that has already learned general features (like edges, textures, and shapes) [1, 3].\n",
        "Lower Computational Costs: Training large models like ResNet or BERT requires substantial computational power (many GPUs/TPUs), which is expensive. Pre-trained models bypass this need [1].\n",
        "Better Performance with Less Data: Pre-trained models leverage knowledge gained from vast, standard datasets (like ImageNet or Wikipedia text). This makes them highly effective for \"transfer learning,\" where they are fine-tuned on smaller, specific datasets, often achieving higher accuracy than a model trained from scratch on that limited data [2, 4].\n",
        "Strong Generalization: The models have already learned robust, general-purpose representations of data, which serve as excellent starting points for new, related tasks [3, 4]."
      ],
      "metadata": {
        "id": "P7mHwty51KFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  How do you install and verify that TensorFlow 2.0 was installed successfully\n",
        "#Open your terminal or command prompt and execute the following command\n",
        "#pip install tensorflow\n",
        "#specific version of TensorFlow 2.x prompt\n",
        "#pip install tensorflow==2.x.x\n",
        "#After the installation completes, you can verify it by running a simple Python script or directly within a Python interpreter.\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "\n",
        "# Optional: Run a simple operation to confirm functionality\n",
        "try:\n",
        "    hello = tf.constant('Hello, TensorFlow!')\n",
        "    tf.print(hello)\n",
        "    print(\"TensorFlow is working correctly.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during TensorFlow operation: {e}\")\n",
        "    python verify_tf.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "p4qezdGs1PQC",
        "outputId": "1a9c913d-ab57-4ab2-dfec-7547d3480f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1500551313.py, line 18)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1500551313.py\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    python verify_tf.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ur6sOvYz-hV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. How can you define a simple function in TensorFlow 2.0 to perform addition2\n",
        "# pip install tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Use the @tf.function decorator to compile the function into a TF graph\n",
        "@tf.function\n",
        "def add_tensors(x, y):\n",
        "    \"\"\"\n",
        "    A TensorFlow function that takes two inputs and returns their sum.\n",
        "    \"\"\"\n",
        "    # Standard TensorFlow operations are used inside the function\n",
        "    z = tf.add(x, y)\n",
        "    # Alternatively, you can use the '+' operator directly\n",
        "    # z = x + y\n",
        "    return z\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# 1. Using TensorFlow Tensors\n",
        "tensor_a = tf.constant([1, 2, 3])\n",
        "tensor_b = tf.constant([4, 5, 6])\n",
        "\n",
        "result_tensor = add_tensors(tensor_a, tensor_b)\n",
        "print(\"Result with Tensors:\", result_tensor.numpy())\n",
        "\n",
        "# 2. Using Python primitives (tf.function handles auto-conversion)\n",
        "int_a = 10\n",
        "int_b = 20\n",
        "\n",
        "result_int = add_tensors(int_a, int_b)\n",
        "print(\"Result with Integers:\", result_int.numpy())\n",
        "\n",
        "# 3. Using NumPy arrays\n",
        "numpy_a = np.array([5.0, 10.0])\n",
        "numpy_b = np.array([1.5, 2.5])\n",
        "\n",
        "result_numpy = add_tensors(numpy_a, numpy_b)\n",
        "print(\"Result with NumPy Arrays:\", result_numpy.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD-HBez92iVt",
        "outputId": "dcd92b89-d753-4e06-adc2-07a3ddde57b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result with Tensors: [5 7 9]\n",
            "Result with Integers: 30\n",
            "Result with NumPy Arrays: [ 6.5 12.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # 3.How can you create a simple neural network in TensorFlow 2.0 with one hidden layer2\n",
        "    # import libraries\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense\n",
        "    #initialize the Sequential model.\n",
        "        model = Sequential()\n",
        "        # add the hidden layer\n",
        "            model.add(Dense(64, input_shape=(input_features,), activation='relu'))\n",
        "                # add the output layer\n",
        "                model.add(Dense(1, activation='sigmoid')) # Example for binary classification\n",
        "                # model compilation\n",
        "                    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "                    #  view moddel summary\n",
        "                       model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Z2H2JNBJ23VG",
        "outputId": "420d2d91-0ef9-48c5-cf0a-e04e977b4eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-2768523514.py, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2768523514.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    model = Sequential()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. How can you visualize the training progress using TensorFlow and Matplotlib\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Define your model (example)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Prepare some dummy data (replace with your actual data)\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "    # Train the model and store the history\n",
        "    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "        # Accessing the metrics\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    train_accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "        # Plotting training and validation loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, 'r', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting training and validation accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracy, 'r', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yBvyzBoU3iO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. How do you install PyTorch and verify the PyTorch installation\n",
        "1. Installing PyTorch:\n",
        "Using pip (for CPU-only).\n",
        "Open your terminal or command prompt and execute:\n",
        "Code\n",
        "\n",
        "    pip install torch torchvision torchaudio\n",
        "This command installs the core PyTorch library, torchvision (for computer vision), and torchaudio (for audio processing).\n",
        "Using conda (recommended, especially for GPU support):\n",
        "Install Anaconda or Miniconda if you haven't already.\n",
        "Open your Anaconda Prompt or terminal.\n",
        "Create a new virtual environment (optional but recommended):\n",
        "Code\n",
        "\n",
        "        conda create -n pytorch_env python=3.x  # Replace 3.x with your desired Python version\n",
        "Activate the environment.\n",
        "Code\n",
        "\n",
        "        conda activate pytorch_env\n",
        "Visit the official PyTorch website (pytorch.org/get-started/locally/) to get the specific conda installation command tailored to your system (OS, CUDA version if using GPU, etc.). Copy and paste the provided command into your activated environment and execute it. An example for CPU-only might look like:\n",
        "Code\n",
        "\n",
        "        conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
        "For GPU support, you would select the appropriate CUDA version on the PyTorch website, and the command would include the CUDA toolkit.\n",
        "2. Verifying the PyTorch Installation:\n",
        "Open a Python interpreter or a Jupyter Notebook within the environment where PyTorch was installed.\n",
        "Import PyTorch and check its version:\n",
        "Python\n",
        "\n",
        "    import torch\n",
        "    print(torch.__version__)\n",
        "This should print the installed PyTorch version.\n",
        "Verify basic functionality by creating a tensor:\n",
        "Python\n",
        "\n",
        "    x = torch.rand(3, 3)\n",
        "    print(x)\n",
        "This will create and print a 3x3 random tensor, indicating that PyTorch is functioning correctly.\n",
        "For GPU installations: Check for CUDA availability:\n",
        "Python\n",
        "\n",
        "    print(torch.cuda.is_available())\n",
        "This should return True if PyTorch is installed with GPU support and a compatible CUDA toolkit is found. You can also check the number of available GPUs:\n",
        "Python\n",
        "\n",
        "    print(torch.cuda.device_count())"
      ],
      "metadata": {
        "id": "t77pt42I6WMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. How do you create a simple neural network in PyTorch2\n",
        "Creating a simple neural network in PyTorch involves defining a class that inherits from torch.nn.Module. This class will contain the layers of your network and define how data flows through them.\n",
        "Here's how to create a simple feedforward neural network: Import necessary libraries.\n",
        "Python\n",
        "\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "Define the Neural Network Class.\n",
        "Create a class that inherits from nn.Module.\n",
        "__init__ method: Initialize the layers of your network here. For a simple feedforward network, you'll typically use nn.Linear for fully connected layers.\n",
        "forward method: Define how the input data x passes through the layers. This method specifies the computational graph of your network.\n",
        "Python\n",
        "\n",
        "    class SimpleNN(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            super(SimpleNN, self).__init__()\n",
        "            self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
        "            self.relu = nn.ReLU()                         # Activation function\n",
        "            self.fc2 = nn.Linear(hidden_size, output_size) # Output layer\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.fc1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.fc2(x)\n",
        "            return x\n",
        "Instantiate the Network.\n",
        "Create an instance of your SimpleNN class, specifying the input, hidden, and output sizes.\n",
        "Python\n",
        "\n",
        "    input_dim = 10\n",
        "    hidden_dim = 20\n",
        "    output_dim = 1\n",
        "\n",
        "    model = SimpleNN(input_dim, hidden_dim, output_dim)\n",
        "    print(model)\n",
        "Forward Pass (Example).\n",
        "You can then pass some dummy data through the network to see its output.\n",
        "Python\n",
        "\n",
        "    dummy_input = torch.randn(1, input_dim) # Create a dummy input tensor\n",
        "    output = model(dummy_input)\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "yZ-RMpo98MCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. How do you define a loss function and optimizer in PyTorch\n",
        "In PyTorch, defining a loss function and an optimizer involves utilizing the torch.nn and torch.optim modules, respectively.\n",
        "Defining a Loss Function:\n",
        "Loss functions quantify the error between a model's predictions and the actual target values. PyTorch provides a variety of built-in loss functions within the torch.nn module.\n",
        "Import torch.nn: Begin by importing the nn module.\n",
        "Python\n",
        "\n",
        "    import torch.nn as nn\n",
        "Instantiate a Loss Function: Choose the appropriate loss function for the task and instantiate it. For example, nn.CrossEntropyLoss() is commonly used for multi-class classification, while nn.MSELoss() is used for regression.\n",
        "Python\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() # For classification\n",
        "    # or\n",
        "    # criterion = nn.MSELoss() # For regression\n",
        "Compute Loss: During the training loop, the loss is computed by passing the model's output and the target values to the instantiated loss function.\n",
        "Python\n",
        "\n",
        "    loss = criterion(outputs, targets)\n",
        "Defining an Optimizer:\n",
        "Optimizers are algorithms that adjust the model's parameters (weights and biases) based on the gradients of the loss function to minimize the loss. PyTorch offers various optimizers in the torch.optim module.\n",
        "Import torch.optim: Import the optim module.\n",
        "Python\n",
        "\n",
        "    import torch.optim as optim\n",
        "Instantiate an Optimizer: Choose an optimizer (e.g., optim.SGD, optim.Adam) and instantiate it, passing the model's parameters and a learning rate.\n",
        "Python\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001) # Stochastic Gradient Descent\n",
        "    # or\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer\n",
        "Optimization Steps: Within the training loop, perform the following steps after computing the loss:\n",
        "Zero the gradients: Clear previous gradients using optimizer.zero_grad().\n",
        "Backpropagation: Compute gradients of the loss with respect to model parameters using loss.backward().\n",
        "Update parameters: Perform a single optimization step using optimizer.step() to update the model's weights.\n"
      ],
      "metadata": {
        "id": "Lw6mmjlU8kUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. How do you implement a custom loss function in PyTorch\n",
        "import torch\n",
        "\n",
        "def custom_mse_loss(predictions, targets):\n",
        "    \"\"\"\n",
        "    Calculates the Mean Squared Error (MSE) between predictions and targets.\n",
        "    \"\"\"\n",
        "    return torch.mean((predictions - targets) ** 2)\n",
        "\n",
        "# Example usage:\n",
        "predictions = torch.randn(10, 1)\n",
        "targets = torch.randn(10, 1)\n",
        "loss = custom_mse_loss(predictions, targets)\n",
        "print(f\"Custom MSE Loss (function): {loss.item()}\")"
      ],
      "metadata": {
        "id": "jYWOK16B80S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. How do you save and load a TensorFlow model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Assuming 'model' is your trained Keras model\n",
        "model.save('my_model.keras')\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = keras.models.load_model('my_model.keras')\n",
        "    model_json = model.to_json()\n",
        "    with open('model_architecture.json', 'w') as json_file:\n",
        "        json_file.write(model_json)\n",
        "            model.save_weights('model_weights.h5')\n",
        "                with open('model_architecture.json', 'r') as json_file:\n",
        "        loaded_model_json = json_file.read()\n",
        "    loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
        "    loaded_model.load_weights('model_weights.h5')"
      ],
      "metadata": {
        "id": "3b9MLVFk92OI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}