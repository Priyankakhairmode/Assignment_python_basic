{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3vp_-hm6Geh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is simple linear regression\n",
        "Regression is a technique of determining statistical relationship between two or more variables where change in one variable(dependent variable) is depend on change in one or more variables(independent variable). simple linear regression is a linear relationship between two variables. An equation of SLR is denoted by y=mx+c\n",
        "where\n",
        "y= dependent variable\n",
        "x=independent variable\n",
        "m= slope of line\n",
        "c= intercept of regression line on y axis\n",
        "\n",
        "2.What are the key assumptions of Simple Linear Regression\n",
        "Assumptions in SLR are as follow:\n",
        "Linearity- Relation between Independent(y) variable and independent(x) variable is linear\n",
        "Homoschedasticity- Difference between Ypredicted and Y actual is constant for every value of x. ie variacnce of the error is constant. and these error terms are independent of each other\n",
        "Non collinearity - no internal correlation between independent variables\n",
        "\n",
        "3.What does the coefficient m represent in the equation Y=mX+c\n",
        "m represent the slope of point of regression line. it is denoted by\n",
        "y2-y1/x2-x1\n",
        "\n",
        "4.What does the intercept c represent in the equation Y=mX+c\n",
        "sLR represented by a straight line. considering this line intersects y at a particular point. distace from origin to the point of intersection on the y axis is called intercept.\n",
        "c= intercept of regression line on y axis\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression\n",
        " in Simple Linear Regression  slope is calculated by the equation given below\n",
        " m= y2-y1/ x2-x1\n",
        " where\n",
        " y2= distace on y axis for data point lets say B\n",
        " x2= distance on x axis for data point lets say B\n",
        " y1= distance on y axis for data point lets say A\n",
        " x1= disatance on x axis for data point lets say A\n",
        "\n",
        " 6.What is the purpose of the least squares method in Simple Linear Regression\n",
        " least square method or ordinary least square method is mathematical technique to find best fitting line or curve for a set of data points.\n",
        " least square method is used to find that combination of m,c where the error is minimum.\n",
        "Least Squared Error = (diff of Y actual- Y predicted )^2\n",
        "\n",
        "7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        " Sum of Suared Error or Mean sum of Suared error = sum of (Yactual-Y predicted)^2/n\n",
        " where n are no of data points\n",
        " This equation  is called  cost function, it is used to find optimal value of m, c\n",
        "\n",
        "8. What is Multiple Linear Regression\n",
        "Multiple Linear Regression is a statistical technique to find the relationship between one dependent variable(y) and multiple independent variable(x)\n",
        "\n",
        "9.What is the main difference between Simple and Multiple Linear Regression\n",
        "the main difference between SLR and MLR is that SLR tecchnique is used to find relationship between one dependent variable and one independent variable unlikely multiple linear regression technique is used to find relationship between one dependent variable and multiple independent variable.\n",
        "MLR consider the enfluence of many factors on single outcome.\n",
        "in SLR only the enfluence of one factore is considered\n",
        "\n",
        "10.What are the key assumptions of Multiple Linear Regression\n",
        "Linearity- Relation between Independent(y) variable and independent(x) variables is linear\n",
        "the observations in the datasets are independent of eaach other\n",
        "Homoschedasticity- Difference between Ypredicted and Y actual is constant for every value of x. ie variacnce of the error is constant. and these error terms are independent of each other. It leads to unreliable hypothesis and inaccurate inferences about the regression coefficients. though the estimate is biased, its very difficult to trust the significance of the result due to inflated standard errors and misleading P values\n",
        "sample is representative of population and sample has enought data points\n",
        "Non collinearity - no internal correlation between independent variables\n",
        "\n",
        "11.  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "heteroscedacity is defined as situation where variance of the error term is not constant across all the values of independent variable.\n",
        "as per assumptions made in MLR difference between Ypredicted and Y actual is constant for every value of x. ie variacnce of the error is constant. and these error terms are independent of each other. but heteroscedasticity affect the P value by causing them to be misleadingly small leading to an incorrect conclusion about the statistical significance of coefficient.\n",
        "\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "VIF- variation inflation factor- this is a technique to find our and remove highly correlated independent variable.\n",
        "PCA- principle component analysis- this is a technique correlated variables and combined to get new variables\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models\n",
        "One Hot Encoding for Nominal Categories with no order\n",
        "Lable Encoding for ordinal categories with order\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression\n",
        "interaction refers to to capture the combined effect of two or more independent variable on the dependent or target variable .\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "in Simple Linear Regression an intercept is a predicted value when the independent variable is zero unlike in multiple Linear regression the intercept represents the predicted value of the dependent variable when all independent variables are set to zero\n",
        "\n",
        "16.  What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "slope in regression analysis is given by m=Y2-Y1/ X2-X1\n",
        "so the change in the dependent variable is depend on every unit changein independent variable\n",
        "Case I +ve slope- dependent variable increses as per unit change in the independent variable\n",
        "CaseII -ve slope - dependent variable decreases as per unit change in the independent variable\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables\n",
        "in a regression model intercept represents the predicted value of the dependent variable when all independent variables are set to zero. thus giving context to the overall trends when no change in the independent variable\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance\n",
        "Rsqure is a statistical measure that represents the proportion of the variance for the dependent variable thats explained by independent variable in a regression model\n",
        "\n",
        "\n",
        "19.How would you interpret a large standard error for a regression coefficient\n",
        "large sample size leads to the a smaller standard error, in addition to this it also examines the issues like outlier, multicollinearity in independent variable\n",
        "therfore a large standard error can be ininterpreted by increasing sample size.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "heteroscedacity is defined as situation where variance of the error term is not constant across all the values of independent variable.\n",
        "as per assumptions made in MLR difference between Ypredicted and Y actual is constant for every value of x. ie variacnce of the error is constant. and these error terms are independent of each other. but heteroscedasticity affect the P value by causing them to be misleadingly small leading to an incorrect conclusion about the statistical significance of coefficient.therfore its important to adress it.\n",
        "\n",
        "\n",
        "\n",
        "21.  What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "if a Multiple Linear Regression model has a high R² but low adjusted R² means the model is a overfit model is likely overfitting the data.\n",
        "\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression\n",
        "scaling is a technique to reduce the magnitude of the variable so that the variables with high magnitude do not enfluence the regression moddel due to its high magnitude. in scaling the range of the values within each variable is adjusted to a comparable scale to facilitate clear idea of the relation between independent variable and dependent variable\n",
        "\n",
        "23. What is polynomial regression\n",
        "polynomial regression is the form of regression in which the relation between the dependent variable y and independent variable x is modelled as the nth degree polunomial in x. eg increrase in the salary of the employees per year can be represented by polynomial equation\n",
        "\n",
        "24. How does polynomial regression differ from linear regression.\n",
        "Linear regression ie the relationship between dependent variable y and independent varianle x and it is represented by a straight line. polynomial regression is a relationship between independent to power of n and dependent variable  is represented by a curved line  \n",
        "\n",
        "25. When is polynomial regression used\n",
        "Polynomial regression is used when the relation between dependent variable and independent variable is non linear. the data points in polynomial regression forms a curved line to capture all data points.\n",
        "\n",
        "26.  What is the general equation for polynomial regression\n",
        "y= b0+b1x+b2x*x+b3x*x*x\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables\n",
        "polynomial regression is applied to multiple variables and it is called multivariate analysis.\n",
        "\n",
        "28. What are the limitations of polynomial regression\n",
        "in polynomial regression a model is designed to capture each data point  and it leads to the suseptibility of the model to become overfit. so if that model has to learn new datapoint it will lead to inaccurate predictions.\n",
        "\n",
        "29.  What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "R suared data, Mean suared error, Visual inspection of data and fitted curve\n",
        "\n",
        "30.  Why is visualization important in polynomial regression\n",
        "visualization in polynomial regression is important, it allows to visually access how well the fitted curve captures the non linear relationship in different data points, it help to understand the issues like overfittin and finally to help the model to predict accuratly the trends and pattern.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2C3jio926-_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#31. How is polynomial regression implemented in Python?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "x=np.array([1,2,3,4,5]).reshape(-1,1)\n",
        "y=np.array([3,7,10,15,16])\n",
        "poly=PolynomialFeatures(degree=2)\n",
        "x_poly=poly.fit_transform(x)\n",
        "Lin_reg=LinearRegression()\n",
        "Lin_reg.fit(x_poly,y)\n",
        "print(Lin_reg.intercept_)\n",
        "print(Lin_reg.coef_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1SOcfaXiX06",
        "outputId": "d0d1c52c-08e5-4e50-f7f3-d2593e67bb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.9999999999999645\n",
            "[ 0.          5.11428571 -0.28571429]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-lPpdGaA6-Z-"
      }
    }
  ]
}